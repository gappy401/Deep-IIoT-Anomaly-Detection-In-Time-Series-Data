{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook implements the LSTM-CNN model  with and without gradient compression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  `load_and_preprocess_data`  function reads the dataset from a specified CSV file and cleans it by removing any rows with missing values. It's the first step in preparing the data for analysis, ensuring that the dataset is complete and ready for further processing.\n",
    "\n",
    "\n",
    "##### `prepare_features_and_labels`  function selects the relevant features (input variables) and the target label (output variable) from the dataset. It transforms the data into a format that can be fed into a machine learning model, separating the input features (`X`) from the target labels (`y`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "# Data Preprocessing Functions\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        file_path (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.dropna()  \n",
    "    return data\n",
    "\n",
    "def prepare_features_and_labels(data):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    features = ['TP2', 'DV_pressure', 'Oil_temperature', 'Motor_current', 'DV_eletric', 'Towers', 'LPS', 'Oil_level', 'Caudal_impulses']\n",
    "    target = 'class'\n",
    "    X = data[features].values\n",
    "    y = data[target].values\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This funtion balances classes as the data is very imbalanced with less than 10% anomaly classes (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_and_sample(X, y, sample_fraction=0.4):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (_type_): _description_\n",
    "        y (_type_): _description_\n",
    "        sample_fraction (float, optional): _description_. Defaults to 0.4.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    sample_size = int(sample_fraction * len(X))\n",
    "    X_sample, y_sample = X[:sample_size], y[:sample_size]\n",
    "    \n",
    "    classes = np.unique(y_sample)\n",
    "    max_samples = max([np.sum(y_sample == cls) for cls in classes])\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        X_cls = X_sample[y_sample == cls]\n",
    "        y_cls = y_sample[y_sample == cls]\n",
    "        \n",
    "        X_balanced.append(X_cls[:max_samples])\n",
    "        y_balanced.append(y_cls[:max_samples])\n",
    "    \n",
    "    X_balanced = np.vstack(X_balanced)\n",
    "    y_balanced = np.hstack(y_balanced)\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "\n",
    "    Raises:\n",
    "        ValueError: _description_\n",
    "        ValueError: _description_\n",
    "        ValueError: _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if 'timestamp' not in data.columns or 'class' not in data.columns:\n",
    "        raise ValueError(\"Data must contain 'timestamp' and 'class' columns.\")\n",
    "    \n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')\n",
    "    data = data.dropna(subset=['timestamp'])\n",
    "    data['month'] = data['timestamp'].dt.to_period('M')\n",
    "    months = data['month'].astype(str).unique()\n",
    "    \n",
    "    if len(months) < 4:\n",
    "        raise ValueError(\"Not enough distinct months to split into global and client data.\")\n",
    "    \n",
    "    months.sort()\n",
    "    first_two_months = months[:2]\n",
    "    last_two_months = months[-2:]\n",
    "    \n",
    "    global_data = data[data['month'].astype(str).isin(first_two_months)]\n",
    "    client1_data = data[data['month'].astype(str) == last_two_months[0]]\n",
    "    client2_data = data[data['month'].astype(str) == last_two_months[1]]\n",
    "    \n",
    "    if global_data.empty or client1_data.empty or client2_data.empty:\n",
    "        raise ValueError(\"One or more of the filtered datasets are empty.\")\n",
    "    \n",
    "    return global_data, client1_data, client2_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient compression method emphasizes **sparsification**, where only the top 0.1% of gradients with the largest absolute values are transmitted, effectively zeroing out 99.9% of the gradients. This approach significantly reduces the amount of data exchanged between edge devices and the central server, enhancing communication efficiency in distributed machine learning systems. By focusing on the most significant gradient updates, this method ensures that essential information is preserved while minimizing the data overhead, making it especially effective for edge computing environments with limited bandwidth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compress_gradients(gradients):\n",
    "    \"\"\"Applies gradient sparsification by keeping only the top 0.1% of the most significant gradients.\"\"\"\n",
    "    compressed_gradients = []\n",
    "    for grad in gradients:\n",
    "        # Flatten the gradient array to apply sparsification\n",
    "        flat_grad = tf.reshape(grad, [-1])\n",
    "        \n",
    "        # Calculate the threshold for top 0.1% gradients\n",
    "        k = int(0.001 * tf.size(flat_grad).numpy())  # 0.1% of the total number of gradients\n",
    "        threshold_value = tf.nn.top_k(tf.abs(flat_grad), k=k).values[-1]  # Get the smallest value in the top 0.1%\n",
    "        \n",
    "        # Sparsify the gradients: zero out values below the threshold\n",
    "        sparsified_grad = tf.where(tf.abs(flat_grad) >= threshold_value, flat_grad, tf.zeros_like(flat_grad))\n",
    "        \n",
    "        # Reshape back to the original gradient shape\n",
    "        sparsified_grad = tf.reshape(sparsified_grad, tf.shape(grad))\n",
    "        \n",
    "        compressed_gradients.append(sparsified_grad)\n",
    "    \n",
    "    return compressed_gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_cnn_lstm_model` function builds a **CNN-LSTM model** designed for sequential data processing. This model integrates convolutional layers to extract spatial features, followed by LSTM layers that capture temporal dependencies. Specifically, it begins with a 1D convolutional layer and max pooling to reduce dimensionality, followed by two LSTM layers for learning temporal sequences. A dense layer with ReLU activation and a dropout layer are used to prevent overfitting, culminating in a final sigmoid-activated dense layer for binary classification. This architecture effectively combines spatial and temporal feature extraction, tailored for tasks involving sequential input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cnn_lstm_model(input_shape):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        input_shape (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = LSTM(50, return_sequences=True)(x)\n",
    "    x = LSTM(50)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simulates a federated learning process where a global model is trained using data from multiple clients. The model is first trained on a global dataset, and then fine-tuned using smaller datasets from individual clients. The function also considers the use of gradient compression, which helps reduce the amount of data that needs to be communicated during training. After fine-tuning, the model's performance is evaluated on the global data to determine its accuracy and other metrics. The results include various performance metrics, such as training time, fine-tuning time, and overall accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(global_data, client1_data, client2_data, use_gradient_compression):\n",
    "    \"\"\"Perform federated learning with CNN-LSTM model, with or without gradient compression.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Prepare global data\n",
    "    print(\"Preparing global data...\")\n",
    "    X_global, y_global = prepare_features_and_labels(global_data)\n",
    "    X_global, y_global = balance_and_sample(X_global, y_global, sample_fraction=0.4)\n",
    "    X_global = X_global[:, np.newaxis, :]\n",
    "    \n",
    "    # Prepare client data\n",
    "    print(\"Preparing client data...\")\n",
    "    X_client1, y_client1 = prepare_features_and_labels(client1_data)\n",
    "    X_client2, y_client2 = prepare_features_and_labels(client2_data)\n",
    "    X_client1, y_client1 = balance_and_sample(X_client1, y_client1, sample_fraction=0.4)\n",
    "    X_client2, y_client2 = balance_and_sample(X_client2, y_client2, sample_fraction=0.4)\n",
    "    X_client1 = X_client1[:, np.newaxis, :]\n",
    "    X_client2 = X_client2[:, np.newaxis, :]\n",
    "    \n",
    "    # Create and train global model\n",
    "    print(\"Creating and training global model...\")\n",
    "    model = create_cnn_lstm_model(input_shape=(X_global.shape[1], X_global.shape[2]))\n",
    "    _, train_time_global = measure_communication_time(model.fit, X_global, y_global, epochs=10, batch_size=32, verbose=2)\n",
    "    print(f\"Time to train global model: {train_time_global:.2f} seconds.\")\n",
    "    \n",
    "    # Fine-tune model on client data\n",
    "    client_data = [\n",
    "        (X_client1, y_client1, \"Client 1\"),\n",
    "        (X_client2, y_client2, \"Client 2\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Fine-tuning model on client data...\")\n",
    "    for X_client, y_client, client_name in tqdm(client_data, desc=\"Clients\", unit=\"client\"):\n",
    "        print(f\"Fine-tuning on {client_name}...\")\n",
    "        \n",
    "        def train_func():\n",
    "            model.fit(X_client, y_client, epochs=5, batch_size=32, verbose=2)\n",
    "        \n",
    "        _, update_time = measure_communication_time(train_func)\n",
    "        print(f\"Time to fine-tune on {client_name}: {update_time:.2f} seconds.\")\n",
    "        \n",
    "        # Compress gradients and simulate communication if required\n",
    "        if use_gradient_compression:\n",
    "            # Assuming a function to apply gradient compression to model weights/gradients\n",
    "            compressed_gradients = compress_gradients(model.get_weights())\n",
    "            # Apply the compressed gradients to the model (This part is a simulation)\n",
    "            model.set_weights([tf.convert_to_tensor(g) for g in compressed_gradients])\n",
    "    \n",
    "    # Evaluate the updated global model\n",
    "    print(\"Evaluating the updated global model...\")\n",
    "    start_time = time.time()\n",
    "    y_global_pred = (model.predict(X_global) > 0.5).astype(int)\n",
    "    report = classification_report(y_global, y_global_pred, output_dict=True)\n",
    "    accuracy = accuracy_score(y_global, y_global_pred)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"Model Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Model Accuracy Score:\", accuracy)\n",
    "    print(f\"Model evaluation completed in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    results['accuracy'] = accuracy\n",
    "    results['classification_report'] = report\n",
    "    results['training_time'] = train_time_global\n",
    "    results['fine_tuning_time'] = update_time\n",
    "    results['evaluation_time'] = elapsed_time\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_communication_time(func, *args, **kwargs):\n",
    "    \"\"\"Measure the time taken for a function to execute.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return result, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing global data...\n",
      "Preparing client data...\n",
      "Creating and training global model...\n",
      "Epoch 1/10\n",
      "5145/5145 - 18s - 3ms/step - accuracy: 0.9733 - loss: 0.0590\n",
      "Epoch 2/10\n",
      "5145/5145 - 14s - 3ms/step - accuracy: 0.9840 - loss: 0.0352\n",
      "Epoch 3/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9896 - loss: 0.0269\n",
      "Epoch 4/10\n",
      "5145/5145 - 13s - 3ms/step - accuracy: 0.9907 - loss: 0.0245\n",
      "Epoch 5/10\n",
      "5145/5145 - 14s - 3ms/step - accuracy: 0.9903 - loss: 0.0259\n",
      "Epoch 6/10\n",
      "5145/5145 - 14s - 3ms/step - accuracy: 0.9920 - loss: 0.0228\n",
      "Epoch 7/10\n",
      "5145/5145 - 14s - 3ms/step - accuracy: 0.9922 - loss: 0.0225\n",
      "Epoch 8/10\n",
      "5145/5145 - 14s - 3ms/step - accuracy: 0.9922 - loss: 0.0225\n",
      "Epoch 9/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9930 - loss: 0.0209\n",
      "Epoch 10/10\n",
      "5145/5145 - 14s - 3ms/step - accuracy: 0.9927 - loss: 0.0222\n",
      "Time to train global model: 142.53 seconds.\n",
      "Fine-tuning model on client data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:   0%|          | 0/2 [00:00<?, ?client/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on Client 1...\n",
      "Epoch 1/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9965 - loss: 0.0127\n",
      "Epoch 2/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9970 - loss: 0.0093\n",
      "Epoch 3/5\n",
      "2707/2707 - 7s - 3ms/step - accuracy: 0.9968 - loss: 0.0107\n",
      "Epoch 4/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9971 - loss: 0.0102\n",
      "Epoch 5/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9972 - loss: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:  50%|█████     | 1/2 [00:38<00:38, 38.08s/client]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fine-tune on Client 1: 38.08 seconds.\n",
      "Fine-tuning on Client 2...\n",
      "Epoch 1/5\n",
      "2783/2783 - 8s - 3ms/step - accuracy: 0.9931 - loss: 0.0209\n",
      "Epoch 2/5\n",
      "2783/2783 - 8s - 3ms/step - accuracy: 0.9955 - loss: 0.0135\n",
      "Epoch 3/5\n",
      "2783/2783 - 9s - 3ms/step - accuracy: 0.9964 - loss: 0.0111\n",
      "Epoch 4/5\n",
      "2783/2783 - 9s - 3ms/step - accuracy: 0.9972 - loss: 0.0092\n",
      "Epoch 5/5\n",
      "2783/2783 - 8s - 3ms/step - accuracy: 0.9973 - loss: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients: 100%|██████████| 2/2 [01:20<00:00, 40.15s/client]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fine-tune on Client 2: 42.22 seconds.\n",
      "Evaluating the updated global model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5145/5145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Classification Report:\n",
      "{'0': {'precision': 0.9733556887973611, 'recall': 1.0, 'f1-score': 0.9864979682305135, 'support': 160227.0}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4386.0}, 'accuracy': 0.9733556887973611, 'macro avg': {'precision': 0.48667784439868056, 'recall': 0.5, 'f1-score': 0.49324898411525675, 'support': 164613.0}, 'weighted avg': {'precision': 0.9474212969141853, 'recall': 0.9733556887973611, 'f1-score': 0.9602134093642087, 'support': 164613.0}}\n",
      "Model Accuracy Score: 0.9733556887973611\n",
      "Model evaluation completed in 10.42 seconds.\n",
      "Preparing global data...\n",
      "Preparing client data...\n",
      "Creating and training global model...\n",
      "Epoch 1/10\n",
      "5145/5145 - 21s - 4ms/step - accuracy: 0.9732 - loss: 0.0590\n",
      "Epoch 2/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9734 - loss: 0.0502\n",
      "Epoch 3/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9760 - loss: 0.0455\n",
      "Epoch 4/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9857 - loss: 0.0322\n",
      "Epoch 5/10\n",
      "5145/5145 - 16s - 3ms/step - accuracy: 0.9884 - loss: 0.0277\n",
      "Epoch 6/10\n",
      "5145/5145 - 16s - 3ms/step - accuracy: 0.9890 - loss: 0.0277\n",
      "Epoch 7/10\n",
      "5145/5145 - 16s - 3ms/step - accuracy: 0.9919 - loss: 0.0227\n",
      "Epoch 8/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9928 - loss: 0.0212\n",
      "Epoch 9/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9932 - loss: 0.0203\n",
      "Epoch 10/10\n",
      "5145/5145 - 15s - 3ms/step - accuracy: 0.9937 - loss: 0.0192\n",
      "Time to train global model: 158.51 seconds.\n",
      "Fine-tuning model on client data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:   0%|          | 0/2 [00:00<?, ?client/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on Client 1...\n",
      "Epoch 1/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9962 - loss: 0.0130\n",
      "Epoch 2/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9969 - loss: 0.0106\n",
      "Epoch 3/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9975 - loss: 0.0089\n",
      "Epoch 4/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9977 - loss: 0.0082\n",
      "Epoch 5/5\n",
      "2707/2707 - 8s - 3ms/step - accuracy: 0.9978 - loss: 0.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clients:   0%|          | 0/2 [00:39<?, ?client/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fine-tune on Client 1: 39.27 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index -1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Perform federated learning with and without gradient compression\u001b[39;00m\n\u001b[0;32m      7\u001b[0m results_without_compression \u001b[38;5;241m=\u001b[39m federated_learning(global_data, client1_data, client2_data, use_gradient_compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m results_with_compression \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient1_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient2_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gradient_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 45\u001b[0m, in \u001b[0;36mfederated_learning\u001b[1;34m(global_data, client1_data, client2_data, use_gradient_compression)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Compress gradients and simulate communication if required\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gradient_compression:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Assuming a function to apply gradient compression to model weights/gradients\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     compressed_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Apply the compressed gradients to the model (This part is a simulation)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_weights([tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m compressed_gradients])\n",
      "Cell \u001b[1;32mIn[38], line 10\u001b[0m, in \u001b[0;36mcompress_gradients\u001b[1;34m(gradients)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate the threshold for top 0.1% gradients\u001b[39;00m\n\u001b[0;32m      9\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39msize(flat_grad)\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# 0.1% of the total number of gradients\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m threshold_value \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get the smallest value in the top 0.1%\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sparsify the gradients: zero out values below the threshold\u001b[39;00m\n\u001b[0;32m     13\u001b[0m sparsified_grad \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mwhere(tf\u001b[38;5;241m.\u001b[39mabs(flat_grad) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold_value, flat_grad, tf\u001b[38;5;241m.\u001b[39mzeros_like(flat_grad))\n",
      "File \u001b[1;32mc:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\nandi\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index -1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "#### Running the Models\n",
    "file_path = 'Metro-Both-Classes.csv'\n",
    "data = load_and_preprocess_data(file_path)\n",
    "global_data, client1_data, client2_data = preprocess_data(data)\n",
    "\n",
    "# Perform federated learning with and without gradient compression\n",
    "results_without_compression = federated_learning(global_data, client1_data, client2_data, use_gradient_compression=False)\n",
    "results_with_compression = federated_learning(global_data, client1_data, client2_data, use_gradient_compression=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
